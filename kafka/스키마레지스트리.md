# 스키마 레지스트리
스키마란 정보를 구성하고 해석하는 것을 도와주는 프레임워크나 개념을 의미한다.  
스키마는 정보를 손쉽게 이해하고 해석하는데 쓰이며, 특히 데이터 베이스의 구조를 정의하고 표현 방법이나 전반적인 명세와 제약 조건을 기술하는 표준 언어로 활용된다.

## 스키마의 개념과 유용성
관계형 데이터베이스는 스키마가 미리 정의되어 있고 데이터베이스에 저장되는 데이터는 스키마에 맞게 저장된다.  

카프카의 데이터 흐름은 브로드캐스트 방식이다.  
카프카는 데이터를 전송하는 프로듀서를 일방적으로 신뢰할 수 밖에 없다.  
따라서 프로듀서 관리자는 카프카 토픽의 데이터를 컨슘하는 관리자에게 반드시 데이터 구조를 설명해야 한다.  
데이터를 컨슘하는 여러 부서에게 데이터에 대한 정확한 정의와 의미를 알려주는 역할을 하는 것이 바로 스키마이다.  

## 카프카와 스키마 레지스트리
카프카에서 스키마를 활용하는 방법은 스키마 레지스트리라는 별도의 애플리케이션을 이용하는 것이다.  
스키마 레지스트리란 글자 그대로 스키마를 등록하고 관리하는 애플리케이션이다.  
아파치 오픈소스 라이선스가 아닌 컨플루언트 커뮤니티 라이선스를 가지고 있어 비상업적인 용도에 한해 스키마 레지스트리를
무료로 사용할 수 있다.

![image](https://github.com/kimj2su/til/assets/95600042/b673f6ea-ea08-49da-af00-86cb3393d459)  

그림에서 알 수 있듯이 스키마 레지스트리는 카프카와 별도로 구성된 독립적 애플리케이션으로서 메시지를 전송하는 프로듀서와 직접 통신하며
카프카로부터 메세지를 꺼내오는 컨슈머와도 직접 통신한다.   
클라이언트들이 스키마 정보를 사용하기 위해서는 프로듀서와 컨슈머, 스키마 레지스트리간에 직접 통신이 이뤄져야한다.  
프로듀서는 스키마 레지스트리에 스키마를 등록하고, 스키마 레지스트리는 프로듀서에 의해 등록된 스키마 정보를 카프카의 내부 토픽에 저장한다.  
프로듀서는 스키마 레지스트리에 등록된 스키마의 ID와 메시지를 카프카로 전송하고 컨슈머는 스키마 ID를 스키마 레지스트리로부터 읽어온 후 프로듀서가
전송한 스키마 ID와 메시지를 조합해 읽을 수 있다. 스키마 레지스트리를 이용하기 위해서는 스키마 레지스트리가 지원하는 데이터 포맷을 사용해야하는데
가장 대표적인 포맷은 에이브로이다.


## 스키마 레지스트리의 에이브로 지원
에이브로는 시스템, 프로그래밍 언어, 프로세싱 프레임워크 사이에서 데이터 교환을 도와주는 오픈소스 직렬화 시스템이다.  
스키마 레지스트리는 에이프로 포맷을 가장 먼저 지원했으며, 최근에는 JSON, 프로토콜 버퍼 포맷도 지원하고 있다.  

대중적으로 많이 가장 많이 사용하는 포맷은 JSON이지만 컨플루언트는 다음과 같은 이유로 에이브로 포맷 사용을 권장한다.
- 에이브로는 JSON과 매핑된다.
- 에이브로는 매우 간결한 데이터 포맷이다.
- JSON은 메시지마다 필드 네임들이 포함되어 전송되므로 효율이 떨어진다.
- 에이브로는 바이너리 형태이므로 매우 빠르다.

```java
{
    "namespace": "student.avro",
    "type": "record",
    "name": "Student",
    "doc": "This is a student schema",
    "fields": [
        {"name": "name", "type": "string", "doc": "Name of the student"},
        {"name": "age", "type": "int", "doc": "Age of the student"},
        {"name": "gpa", "type": "float", "doc": "GPA of the student"}
    ]

```
- namespace: 이름을 식별하는 문자열
- type: 에이브로는  record, enums, arrays, maps 등을 지원하며 여기서는 record 타입으로 정의
- doc : 사용자들에게 이 스키마 정의 대한 설명 제공(주석)
- name: 레코드의 이름을 나타내는 문자열로서, 필수 값이다.
- fields: JSON 배열로서, 필드들의 리스트를 뜻한다.
- name: 필드의 이름
  - type: boolean, int, long, string 등의 데이터 타입 정의
  - doc: 사용자들에게 이 스키마 정의 대한 설명 제공(주석)

JSON과 달리 에이브로는 데이터 필드마다 데이터 타입을 정의할 수 있고 doc를 이용해 각 필드의 의미를 데이터를 사용하고자 하는 사용자들에게 정확하게 전달할 수 있다.  
이러한 doc 기능을 활용하면 데이터 필드를 정의한 엑셀 문서나 위키 페이지 등을 각 부서가 서로 공유하지 않아도 된다.  

## 에이브로 공식 문서
https://avro.apache.org/docs/current/spec.html

### 스키마 레지스트리 API

스키마 레지스트리는 HTTP 기반으로 통신이 이뤄지며, 사용자들이 유연성과 편의성을 위해 스키마 레지스트리의 주요 기능들에 대한 API를 제공한다.  
스키마 레지스트리를 직접 사용하는 사용자들은 이러한 API를 통해 애플리케이션을 간소화할 수 있으며 시간도 절약할 수 있다.

| 옵션                             | 설명|
|--------------------------------|---|
| GET /schemas                   | 현재 스키마 레지스트리에 등록된 전체 스키마 리스트 조회|
| GET /schemas/ids/{id}          | 스키마 ID로 조회 |
| GET /schemas/ids/is/versions   | 스키마 ID의 버전|
| GET / subjects                 | 스키마 레지스트리에 등록된 subject 리스트 <br> subject는 토픽이름 - key, 토픽 이름 - value 형태로 쓰임|
| GET /subjects/서브젝트 이름/versions | 특정 서브젝트의 버전 리스트 조회|
| GET /config                    | 전역으로 설정된 호환성 레벨 조회|
| GET /config/서브젝트이름 | 서브젝트에 설정된 호환성 조회|
| DELETE /subjects/서브젝트 이름 | 특정 서브젝트 전체 삭제|
| DELETE /subjects/서브젝트 이름/versions/버전 | 특정 서브젝트에서 특정 버전만 삭제|



## 스키마 레지스트리와 클라이언트 동작
카프카의 모델은 펍/섭 모델로서 프로듀서와 컨슈머는 직접 통신을 주고 받지 않는다.  
스키마가 정의되어 있는 메시지를 컨슈머가 읽기 위해서는 프로듀서가 정의한 스키마 정보를 알아야 한다.  
프로듀서와 컨슈머가 직접 통신하지 않는데 컨슈머는 어떻게 프로듀서가 정의한 스키마 정보를 알 수 있을까?  

컨플루언트 공식 홈페이지에서 제공하는 스키마 레지스트리의 동작 방식

![image](https://github.com/kimj2su/til/assets/95600042/b116f245-eaf1-4e33-a385-d2593b168d7d)

1. 에이브로 프로듀서는 컨플루언트에서 제공하는 io.confluent.kafka.serializers.KafkaAvroSerializer라는 새로운 직렬화를 사용해 스키마 레지스트리의 스키마가 유효한지 여부를 확인한다. 만약 스키마가 확인되지 않으면 에이브로 프로듀서는 스키마를 등록하고 캐시한다.
2. 스키마 레지스트리는 현 스키마가 저장소에 저장된 스키마와 동일한 것인지, 진화한 스키마인지 확인한다. 스키마 레지스트리 자체적으로 각 스키마에 대해 고유 ID를 할당하게 된다. 이 ID는 순차적으로 1씩 증가하지만 반드시 연속적이진 않다. 스키마에 문제가 없다면 스키마 레지스트리는 프로듀서에게 고유한 ID를 응답한다.
3. 프로듀서는 스키마 레지스트리로부터 받은 스키마 ID를 참고해 메시지를 카프카로 전송한다. 이때 프로듀서는 스키마의 전체 내용이 아닌 오로지 메시지와 스키마ID만 보낸다. JSON은 키-벨류 형태로 전체 메시지를 전송해야하지만, 에이브로를 사용하면 프로듀서가 스키마 ID와 벨류만 메시지로 보내게 되어 카프카로 전송하는 전체 메시지의 크기를 줄일 수 있으며, 이는 JSON보다 에이브로를 사용하는 편이 더 효율적인 이유이기도 하다.
4. 에이브로 컨슈머는 스키마 ID로 컨프루언트에서 제공하는 io.confluent.kafka.serializers.KafkaAvroDeserializer를 사용해 카프카의 토픽에 저장된 메시지를 읽는다. 이때 컨슈머가 스키마 ID를 갖고 있지 않다면 스키마 레지스트리로부터 가져온다.

## 스키마 레지스트리 활용
여기서 자바 애플리케이션을 활용하고 카프카 환경은 도커로 구성한다.  
docker-compose.yml 파일에 카프카 환경을 구성하고 docker compose up -d 로 실행  


AvroProducer1.java
```java
public class AvroProducer1 {
    public static void main(String[] args) {
        // Avro 스키마 정의
        String schemaString = "{"
                + "\"namespace\": \"student.avro\","
                + "\"type\": \"record\","
                + "\"doc\": \"This is an example of Avro.\","
                + "\"name\": \"Student\","
                + "\"fields\": ["
                + "{\"name\": \"name\", \"type\": [\"null\", \"string\"], \"default\": null, \"doc\": \"Name of the student\"},"
                + "{\"name\": \"class\", \"type\": \"int\", \"default\": 1, \"doc\": \"Class of the student\"}"
                + "]"
                + "}";

        Schema schema = new Schema.Parser().parse(schemaString);

        // Kafka producer properties 설정
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "http://localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class.getName());
        props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, "http://localhost:8081");

        // KafkaProducer 생성
        KafkaProducer<String, GenericRecord> producer = new KafkaProducer<>(props);

        // Avro 레코드 생성
        GenericRecord student = new GenericData.Record(schema);
        student.put("name", "Peter");
        student.put("class", 1);

        // ProducerRecord 생성
        ProducerRecord<String, GenericRecord> record = new ProducerRecord<>("peter-avro2", student);

        // 메시지 전송 및 결과 확인
        try {
            RecordMetadata metadata = producer.send(record).get();
            System.out.printf("Message sent to topic:%s partition:%s  offset:%s%n",
                    metadata.topic(), metadata.partition(), metadata.offset());
        } catch (ExecutionException | InterruptedException e) {
            e.printStackTrace();
        }

        // Producer 닫기
        producer.close();
    }
}
```
여기서 스키마에 default 값을 추가했는데 이렇게 해두면 프로듀서에서 필드의 name 또는 class에 별도의 값을 입력하지 않는 경우 default 값이 들어간다.  
추후에 변경될 수도 있으므로 사전에 default 값을 추가하는것을 권장한다.

Student.avsc
```json
{
  "namespace": "student.avro",
  "type": "record",
  "name": "Student",
  "doc": "This is an example of Avro.",
  "fields": [
    {"name": "name", "type": ["null", "string"], "default": null, "doc": "name"},
    {"name": "class", "type": "int", "default": 1, "doc": "Class of the student"}
  ]
}
```
AvroConsumer1.java
```java
public class AvroConsumer {

    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "http//localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "avro-consumer-group");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class.getName());
        props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, "http://localhost:8081");
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        props.put("specific.avro.reader", "true");

        // KafkaConsumer 생성
        KafkaConsumer<String, Student> consumer = new KafkaConsumer<>(props);

        // 구독할 토픽 설정
        consumer.subscribe(Collections.singletonList("peter-avro2"));

        // 메시지 소비 루프
        try {
            while (true) {
                ConsumerRecords<String, Student> records = consumer.poll(Duration.ofMillis(100));
                for (ConsumerRecord<String, Student> record : records) {
                    System.out.printf("Consumed message: topic = %s, partition = %s, offset = %s, key = %s, value = %s%n",
                            record.topic(), record.partition(), record.offset(), record.key(), record.value());
                }
            }
        } finally {
            consumer.close();
        }
    }
}
```
위의 코드는 peter-avro2 토픽을 컨슘하고, 컨슈머 그룹아이디는 avro-consumer-group로 처음 오프셋부터 메시지를 읽어온다.

스키마 레지스트리 API를 통해 스키마가 스키마레지스트리에 잘 기록되었는지 확인한다.
```shell
curl http://localhost:8081/schemas
http -v ":8081/schemas"
```

여기서 스키마의 진화가 필요한 경우가 있다.  
현재 예제는 이름(name)과 반(class)만 정의했다. 한 반에 속한 학생 수가 소수인 경우에는 학생 이름이 중복될 확률이 적지만, 학생수가 점차 증가하면
한 반에 이름이 중복되는 현상이 발생할 수 있다.  
따라서 name 필드를 삭제한 다음, first_name과 last_name이라는 2개의 필드를 추가하는 식으로 변경해야 한다.  
만약 스키마 레지스트리가 없다면 변경사항을 담당자들에게 공유하고 프로듀서와 컨슈머 코드 변경을 비롯한 클라이언트를 중지한후 배포해야 한다.  

AvroProducer2.java
```java
public class AvroProducer2 {
    public static void main(String[] args) {

        // Avro 스키마 정의
        String schemaString = "{"
                + "\"namespace\": \"student.avro\","
                + "\"type\": \"record\","
                + "\"doc\": \"This is an example of Avro.\","
                + "\"name\": \"Student\","
                + "\"fields\": ["
                + "{\"name\": \"first_name\", \"type\": [\"null\", \"string\"], \"default\": null, \"doc\": \"First name of the student\"},"
                + "{\"name\": \"last_name\", \"type\": [\"null\", \"string\"], \"default\": null, \"doc\": \"Last name of the student\"},"
                + "{\"name\": \"class\", \"type\": \"int\", \"default\": 1, \"doc\": \"Class of the student\"}"
                + "]"
                + "}";

        Schema schema = new Schema.Parser().parse(schemaString);

        // Kafka producer properties 설정
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "http://localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class.getName());
        props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, "http://localhost:8081");

        // KafkaProducer 생성
        KafkaProducer<String, GenericRecord> producer = new KafkaProducer<>(props);

        // Avro 레코드 생성
        GenericRecord student = new GenericData.Record(schema);
        student.put("first_name", "Peter");
        student.put("last_name", "Parker");
        student.put("class", 1);

        // ProducerRecord 생성
        ProducerRecord<String, GenericRecord> record = new ProducerRecord<>("peter-avro2", student);

        // 메시지 전송 및 결과 확인
        try {
            RecordMetadata metadata = producer.send(record).get();
            System.out.printf("Message sent to topic:%s partition:%s  offset:%s%n",
                    metadata.topic(), metadata.partition(), metadata.offset());
        } catch (ExecutionException | InterruptedException e) {
            e.printStackTrace();
        }

        // Producer 닫기
        producer.close();
    }
}
```
위에서 name 필드를 삭제하고 first_name과 last_name을 추가하였다.  
peter-avro2 토픽으로 전송할 메시지의 내용은 class는 1반, first_name은 Peter, last_name은 Parker로 변경되었다.  
여기서 중요한 사실은 컨슈머가 계속 동작중인 상황에서 스키마 변경 작업이 수행됐다.  
물론 컨슈머는 진화된 스키마 정보가 반영되지 않아 프로듀서가 전송한 first_name과 last_name 필드의 내용은 알 수 없어 null이 나오지만
스키마 변경으로 인해 메시지 형식이 달라졌음에도 에러는 발생하지 않았다.  
이처럼 스키마 레지스트리를 이용하면 메시지의 변경에도 유연하게 대응할 수 있다.  

이제 진화된 스키마를 컨슈머에 적용하고 스키마 변경 전후의 변화를 확인하기 위해 토픽의 첫번째 부터 메시지를 가져오고  
가져오기 위해 컨슈머 그룹아이디를 새로운 값으로 변경한다.

Student.avsc
```json
{
  "namespace": "student.avro",
  "type": "record",
  "name": "Student",
  "doc": "This is an example of Avro.",
  "fields": [
    {"name": "first_name", "type": ["null", "string"], "default": null, "doc": "Last name of the student"},
    {"name": "last_name", "type": ["null", "string"], "default": null, "doc": "Last name of the student"},
    {"name": "class", "type": "int", "default": 1, "doc": "Class of the student"}
  ]
}
```
AvroConsumer2.java
```java
public class AvroConsumer2 {

    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "http//localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "avro-consumer-group2");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class.getName());
        props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, "http://localhost:8081");
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        props.put("specific.avro.reader", "true");

        // KafkaConsumer 생성
        KafkaConsumer<String, Student> consumer = new KafkaConsumer<>(props);

        // 구독할 토픽 설정
        consumer.subscribe(Collections.singletonList("peter-avro2"));

        // 메시지 소비 루프
        try {
            while (true) {
                ConsumerRecords<String, Student> records = consumer.poll(Duration.ofMillis(100));
                for (ConsumerRecord<String, Student> record : records) {
                    System.out.printf("Consumed message: topic = %s, partition = %s, offset = %s, key = %s, value = %s%n",
                            record.topic(), record.partition(), record.offset(), record.key(), record.value());
                }
            }
        } finally {
            consumer.close();
        }
    }
}
```

![image](https://github.com/kimj2su/til/assets/95600042/e18e4621-9b21-421c-beda-7599cf9d3941)
첫번째 메시지는 스키마가 진화하기전 프로듀서가 보낸 메시지이고, 두 번째 메시지는 스키마 진화 후 프로듀서가 보낸 메시지이다.  
진화된 스키마를 컨슈머에 적용했음에도 컨슈머는 스키마가 다른 첫 번째 메시지도 잘 가져온다.  
스키마 레지스트리를 도입하지 않는 일반적인 상황이라면 메시지 형식이 바뀔 때마다 컨슈머에서는 문자열 파싱에러가 발생해 컨슈머 프로세스가 종료되기 때문에 코드를 수정해야한다.  
하지만 스키마 레지스트리를 사용하면 프로듀서가 보내는 메시지의 스키마가 변경되더라도 컨슈머는 오류 없이 모든 메시지를 잘 읽으 수 있다.  
그리고 컨슈머 작업이 가능한 시간에 맞추어 새로운 스키마를 적용한다면 아무런 이슈 없이 처리가 가능하다.  
이런 이득과 장점때문에 스키마 레지스트리를 도입해야한다.  

```json
curl http://localhost:8081/schemas
http -v ":8081/schemas"

[
  {
    "id": 1,
    "schema": "{\"type\":\"record\",\"name\":\"Student\",\"namespace\":\"student.avro\",\"doc\":\"This is an example of Avro.\",\"fields\":[{\"name\":\"name\",\"type\":[\"null\",\"string\"],\"doc\":\"Name of the student\",\"default\":null},{\"name\":\"class\",\"type\":\"int\",\"doc\":\"Class of the student\",\"default\":1}]}",
    "subject": "peter-avro2-value",
    "version": 1
  },
  {
    "id": 2,
    "schema": "{\"type\":\"record\",\"name\":\"Student\",\"namespace\":\"student.avro\",\"doc\":\"This is an example of Avro.\",\"fields\":[{\"name\":\"first_name\",\"type\":[\"null\",\"string\"],\"doc\":\"First name of the student\",\"default\":null},{\"name\":\"last_name\",\"type\":[\"null\",\"string\"],\"doc\":\"Last name of the student\",\"default\":null},{\"name\":\"class\",\"type\":\"int\",\"doc\":\"Class of the student\",\"default\":1}]}",
    "subject": "peter-avro2-value",
    "version": 2
  }
]

```
스키마 정보를 확인해보면 진화된 스키마 내용이 반영됐고 버전이 2로 바뀐것을 확인할 수 있다.

```json
curl http://localhost:8081/subjects/peter-avro2-value/versions
```

```json
[
  1,
  2
]
```
peter-avro2-value 서브젝트에는 버전 1과 2가 등록되어 있다.  
처음에는 스키마를 정의할 때 변화가 없을 것이라고 예상하지만 차음 시간이 흐르면서 시스템의 변화 요구 등으로 인해 데이터나 스키마의 진화가 필요한 상황이 생긴다.  
